{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from xmlrpc.server import SimpleXMLRPCServer, SimpleXMLRPCRequestHandler\n",
    "import socketserver\n",
    "import xmlrpc.client\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combos: 720\n"
     ]
    }
   ],
   "source": [
    "HY_COMBOS = {\n",
    "     # each element of conv_layers should be tuple:\n",
    "     #(num filters, filtersize tuple, poolsize tuple OR none if not pooling)\n",
    "    \"conv_layers\": [[(32,(3,3),(2, 2)), (10,(3,3),(2,2))], [(32,(3,3),(2, 2))],[]],\n",
    "    # this is a list of sizes for each dense layer\n",
    "    # do not include the final prediction layer here\n",
    "    \"dense_layers\" : [[50,10,5], [40,20],[10],[]],\n",
    "    # apply the SAME activation and dropout to every conv and dense layer \n",
    "    # (except for final output which has hardcoded softmax activation w/ no dropout)\n",
    "    \"activation\": [\"relu\", \"sigmoid\"], \n",
    "    \"dropout\":  np.arange(0,.5,.1),\n",
    "    # l2 regularization for weights\n",
    "    \"k_reg\": [0,.001],\n",
    "    # optimization hyperparameters\n",
    "    \"learning_rate\": [.01],\n",
    "    \"grad_clip_norm\": np.arange(0,.5,.2)}\n",
    "    \n",
    "    \n",
    "hy_list = list(ParameterGrid(HY_COMBOS)) \n",
    "print(\"Total combos:\", len(hy_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = np.empty([len(hy_list), max_epochs])\n",
    "losses.fill(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(epoch_num, model_num, loss):\n",
    "    losses[model_num, epoch_num] = loss\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if we pick best model based on best performance across training\n",
    "best_model = np.where(losses == np.nanmin(losses))[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if we pick best model based on the last training epoch\n",
    "def get_last_loss(row):\n",
    "    if len(np.where(~np.isnan(row))[0]):\n",
    "        return (row[np.where(~np.isnan(row))[0][-1]])\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "lastlosses = np.array([get_last_loss(row) for row in losses])\n",
    "best_model = np.where(lastlosses == np.nanmin(lastlosses))[0][0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
